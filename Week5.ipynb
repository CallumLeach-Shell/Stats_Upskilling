{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Bayesian Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Useful reference: An Introduction to Bayesian Thinking (statswithr.github.io)\n",
    " \n",
    "Reading materials: \n",
    "https://towardsdatascience.com/how-bayes-theorem-helped-win-the-second-world-war-7f3be5f4676c\n",
    "Can A 250-Year-Old Mathematical Theorem Find A Missing Plane? : The Two-Way : NPR\n",
    "https://www.colorado.edu/amath/sites/default/files/attached-files/vallverdu08.pdf\n",
    "\n",
    "### Exercise 0:\n",
    " \n",
    "1. What is Bayes Rule?\n",
    "2. What are the terms?\n",
    "3. How is it different to likelihood inference?\n",
    "\n",
    "\n",
    "### Exercise 1:\n",
    "We are interested in determining the probability that someone has covid given that the rapid test is positive.\n",
    " \n",
    "We know the following pieces of information:\n",
    "* The probability the rapid test is positive given that the person does have covid is 0.95.\n",
    "* The probability the rapid test is negative given that the person does not have covid is 0.90.\n",
    "* The probability of the general population having covid is 0.20.\n",
    "* What do each of the terms in Bayes rule refer to us calculating? \n",
    "* Why is it important to calculate probabilities like the one stated above?\n",
    " \n",
    "### Exercise 2:\n",
    "Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n",
    " \n",
    "Deriving posterior distributions \n",
    " \n",
    "For the following cases \n",
    "* Prior: f(p)~Beta(a, b) Likelihood: f(x| p)~Binomial(n, p)\n",
    "* Prior: f(mu)Normal(a, b) Likelihood: f(x| mu, sigma)~Normal(mu, sigma) (sigma known)\n",
    "* Prior: f(mu)~Laplace(a, b) Likelihood: f(x| mu, sigma)~Normal(mu, sigma) (sigma known)\n",
    " \n",
    "1. Why might we use a Laplace instead of a Normal distribution?\n",
    "2. Why are conjugate priors useful?\n",
    "3. How do we solve the cases where we do not have closed form? \n",
    "4. If we take the regression example from the previous week 4 how can we estimate the parameters?\n",
    "5. How can we determine the priors?\n",
    "6. Why might the distribution of the priors be different?\n",
    "7. Using a Normal prior for Beta and an inverse Gamma prior for sigma^2 can you derive the posterior distribution?\n",
    "8. How does this case link to Ridge Regression?\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem describes the probability of an event happening, when there is prior knowledge of related events. It has the following formulation:\n",
    "\n",
    "1. What is Bayes rule: $P(A|B) = \\frac{P(B | A)P(A)}{P(B)}$\n",
    "   1. Note, this follows from the definition of conditional probability, $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "2. What are the terms?:\n",
    "   1. $P(A|B)$ is the conditional probability the $A$ happens given that $B$ is true.\n",
    "   2. $P(B|A)$ is the conditional probability that $B$ happens given that $A$ is true. Can be interpreted as the likelihood of $A$ given a fixed $B$. $P(B|A) = L(A|B)$\n",
    "   3. $P(A)$, $P(B)$ are the probabilties of observing $A$ and $B$ without any prior conditions, these are known as the *prior* and *marginal* distributions respectivly.\n",
    "\n",
    "3. How is it different to Likelihood inference?:\n",
    "   1. Likelihood inference relies solely on the observed data, whilst Bayesian Inference uses prior knowledge to improve inference results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "In the following exercise, we want to determine the probability someone has covid given the rapid flow test is positive. This can be expressed in Bayes form as:\n",
    "\n",
    "$P(Has Covid | test positive) = \\frac{P(test positive | has covid)P(Has covid)}{P(test positive)}$\n",
    "\n",
    "We know that:\n",
    "1. $P(test positive | Has covid) = 0.95$\n",
    "2. $P(\\neg(test positive | Has covid)) = 0.90$\n",
    "3. $P(Has covid) = 0.2$\n",
    "\n",
    "All we need is the $P(test positive)$ marginal distribution, to do this we note:\n",
    "\n",
    "$P(test positive) = P(test positive \\cap Has covid) + P(test positive \\cap \\neg Has covid)$\n",
    "\n",
    "Then using the definition of conditional probability,\n",
    "\n",
    "$P(test positive) = P(Has covid)P(test positive | has covid) + P(\\neg Has covid)P(test positive | \\neg Has covid)$\n",
    "\n",
    "$P(test positive) = 0.2*0.95 + (1-0.2)*(1-0.9) = 0.27$\n",
    "\n",
    "Thus substituting into Bayes formula:\n",
    "\n",
    "$P(Has Covid | test positive) = \\frac{0.95*0.2}{0.27} = 0.704 \\approx 71 \\%$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "We define $C$ to be the event that there is a car behind door 1, and $E$ the event that monty opens a door with a goat behind it.\n",
    "Then we want to find $P(C|E)$, which given Bayes theory has the form:\n",
    "\n",
    "$P(C|E) = \\frac{P(E|C)P(C)}{P(E)}$\n",
    "\n",
    "We know that:\n",
    "* P(E|C) = 1, the probability that monty opens a door with a goat behind it given there is a car behind door 1, this will be 1 as he always picks the door with a goat. (his prior knowledge).\n",
    "* P(C) = \\frac{1}{3}, there a three doors to choose from and without prior knowledge a 1 in three chance to pick a door with a car.\n",
    "* P(E) = 1, as monty will always pick a door with a goat behind it.\n",
    "\n",
    "$P(C|E) = \\frac{\\frac{1}{3}*1}{1} = \\frac{1}{3}$\n",
    "\n",
    "Then defining the probability that the remaining door has a car behind it $P(O)$. We know that:\n",
    "\n",
    "$P(O|E) + P(C|E) = 1$ as either the car is behind door 1 or its behind the remaining door.\n",
    "\n",
    "so $P(O|E) = 1-\\frac{1}{3} = \\frac{2}{3}$\n",
    "\n",
    "### Deriving Posterior Distributions\n",
    "\n",
    "Suppose X, Y are two RVs having joint PDF or PMF $f_{X,Y}(x,y)$, then the *marginal distribution* of $X$ is given by:\n",
    "$f_X(x) = \\int f_{X,Y}(x,y)\\,dy$ in the continuous case. Or by the PMF\n",
    "$f_X(x)= \\sum{f_{X,Y}(x,y)}$ in the discrete case. This describes the probability distribution of $X$ alone.\n",
    "\n",
    "**Conditional Distribution:** of $Y$ given $X=x$ is defined by the PDF or PMF\n",
    "\n",
    "$f_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}$\n",
    "\n",
    "Doing a similar for the other variable, we can decompose the joint PDF as follows:\n",
    "\n",
    "$f_{X,Y}(x,y) = f_{Y|X}(y|x)f_X(x) = f_{X|Y}(x|y)f_Y(y)$\n",
    "\n",
    "In Bayesian analysis, before data is observed, the unknow parameter is modeled as a random variable $\\Theta$ with probability distribution $f_\\Theta(\\theta)$, called the *prior distribution*.\n",
    "\n",
    "When we have observed data $X$, we can model the joint distribution as\n",
    "\n",
    "$f_{X,\\Theta}(x, \\theta) = f_{X|\\Theta}(x,\\theta)f_{\\Theta}(\\theta)$\n",
    "\n",
    "and the marginal distribution of $X$ (continuous case) follows:\n",
    "\n",
    "$f_X(x) = \\int f_{X,\\Theta}(x, \\theta)\\, d\\theta = \\int f_{X|\\Theta}(x|\\theta)f_{\\Theta}(\\theta)\\, d\\theta$\n",
    "\n",
    "Hence the conditional distribution (**Posterior distribution**) of $\\Theta$ given $X=x$ is\n",
    "\n",
    "$f_{\\Theta|X}(\\theta|x) = \\frac{f_{X,\\Theta}(x,\\theta)}{f_X(x)} = \\frac{f_{X|\\Theta}(x|\\theta)f_\\Theta(\\theta)}{ \\int f_{X|\\Theta}(x|\\theta')f_{\\Theta}(\\theta')\\, d\\theta' }$\n",
    "\n",
    "\n",
    "Ususally summarised as $f_{\\Theta|X}(\\theta|x) \\propto f_{X|\\Theta}(x,\\theta)f_\\Theta(\\theta)$ i.e\n",
    "\n",
    "$Posterior density \\propto Likelihood * Prior density$\n",
    "\n",
    "1. Prior: f(p)~Beta(a, b) Likelihood: f(x| p)~Binomial(n, p)\n",
    "2. Prior: f(mu)Normal(a, b) Likelihood: f(x| mu, sigma)~Normal(mu, sigma) (sigma known)\n",
    "3. Prior: f(mu)~Laplace(a, b) Likelihood: f(x| mu, sigma)~Normal(mu, sigma) (sigma known)\n",
    "\n",
    "1. Posterior $f_{P|X}(p|x) = f_{X|P}(x|p)f_P(p) = Binomial(n, p) * Beta(a, b)$\n",
    "\n",
    "$$Binomial(n, p) = {n \\choose k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "$$Beta(\\alpha, \\beta) = \\frac{x^{\\alpha -1}(1-x)^{\\beta -1}}{B(\\alpha, \\beta)}\\quad \\text{Where: }B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
